{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXkuh5_mj3Wx"
      },
      "source": [
        "##Word2vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pJqjzaGlQnT"
      },
      "source": [
        "Word representation methods from the last lab\n",
        "\n",
        "- Bag of Words\n",
        "- TF-IDF\n",
        "\n",
        "Limitations of these representations\n",
        "\n",
        "- High-dimensional\n",
        "- Sparse\n",
        "- No info about words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GPodGvnlyZX"
      },
      "source": [
        "Word2vec Paper [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkQ87w_smCcg"
      },
      "source": [
        "Word2Vec is a shallow, two-layer neural network which is trained to reconstruct linguistic contexts of words.\n",
        "\n",
        "It takes as its input a large corpus of words and produces a vector space, with each unique word in the corpus being assigned a corresponding vector in the space.\n",
        "\n",
        "\n",
        "Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
        "\n",
        "Example:    \n",
        "The **kid** studies mathematics.\n",
        "\n",
        "The **child** studies mathematics.\n",
        "\n",
        "![embedding](https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7oue8cDnY1_"
      },
      "source": [
        "###Methods for building the Word2vec model\n",
        "\n",
        "![cbow-skip-gram](https://miro.medium.com/max/1400/1*cuOmGT7NevP9oJFJfVpRKA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KUil7cBnY4u"
      },
      "source": [
        "###Continuous Bag-of-Words (CBOW)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb12NPvv3Zww"
      },
      "source": [
        "CBOW predicts target words from the surrounding context words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfz-eLcNnY7f"
      },
      "source": [
        "![cbow](https://1.bp.blogspot.com/-nZFc7P6o3Yc/XQo2cYPM_ZI/AAAAAAAABxM/XBqYSa06oyQ_sxQzPcgnUxb5msRwDrJrQCLcBGAs/s1600/image001.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZpZNbVynY91"
      },
      "source": [
        "###Skip-gram\n",
        "\n",
        "Skip-gram predicts surrounding context words from the target words.\n",
        "\n",
        "![skip-gram](https://i.stack.imgur.com/fYhXF.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GGcsuG3s9_"
      },
      "source": [
        "##Architecture\n",
        "\n",
        "The words are fed as one-hot vectors ( vector of the same length as the vocabulary, filled with zeros except at the index that represents the word we want to represent, which is assigned ‚Äú1‚Äù.)\n",
        "\n",
        "The hidden layer is a standard fully-connected (Dense) layer whose weights are the word embeddings.\n",
        "\n",
        "The output layer outputs probabilities for the target words from the vocabulary.\n",
        "\n",
        "The goal of this neural network is to learn the weights for the hidden layer matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model](https://miro.medium.com/max/1400/1*tmyks7pjdwxODh5-gL3FHQ.png)\n",
        "\n",
        "High-level illustration of the architecture\n",
        "\n",
        "![model2](https://i.imgur.com/CBuZay5.png)\n",
        "\n",
        "The rows of the hidden layer weight matrix, are actually the word vectors (word embeddings).\n",
        "\n",
        "\n",
        "![hidden-layer](https://i.imgur.com/v6VqHad.png)\n",
        "\n",
        "The hidden layer operates as a lookup table. The output of the hidden layer is just the ‚Äúword vector‚Äù for the input word.\n",
        "\n",
        "More concretely, if you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the ‚Äò1‚Äô.\n",
        "\n",
        "![vector](https://i.imgur.com/EYhcA5S.png)"
      ],
      "metadata": {
        "id": "UbyOIWcPWsfy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZgTsN3Pmye0"
      },
      "source": [
        "###Semantic and syntactic relationships\n",
        "\n",
        "If different words are similar in context, then Word2Vec should have similar outputs when these words are passed as inputs, and in-order to have a similar outputs, the computed word vectors (in the hidden layer) for these words have to be similar, thus Word2Vec is motivated to learn similar word vectors for words in similar context.\n",
        "\n",
        "Word2Vec is able to capture multiple different degrees of similarity between words, such that semantic and syntactic patterns can be reproduced using vector arithmetic.\n",
        "\n",
        "![w2vec](https://i.imgur.com/I66L7No.png)\n",
        "\n",
        "![w2vec2](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/linear-relationships.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Now1mZyS_dYe"
      },
      "source": [
        "**Skip-gram** - works well with a small amount of the training data, represents well even rare words or phrases\n",
        "\n",
        "**CBOW** - several times faster to train than the skip-gram, slightly better accuracy for the frequent words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSzy26Qw_dc5"
      },
      "source": [
        "###Word2vec embeddings in Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuFIQwiLQkB5"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BUnYjhKAWoI"
      },
      "source": [
        "Gensim has multiple vector representations for words: word2vec, fasttext, glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7rV7zCyAHdJ",
        "outputId": "b7d9d77d-c425-40a2-9e9b-d7aab4bae297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ],
      "source": [
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo0dnaGaAesc"
      },
      "source": [
        "Downloading the word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XBIE2c_Aifx"
      },
      "outputs": [],
      "source": [
        "word2vec = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acjBn5fQRoxb",
        "outputId": "b20c019d-3001-427b-8c44-76e77d6ca8bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
              "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
              "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
              "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec['cat'][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6-ySA4OBK3P",
        "outputId": "e1d54245-4682-42e7-85d3-4c114e8d68ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.25689757"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec.similarity('dog', 'house')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDMjYXayBNB1",
        "outputId": "61cb7dc1-08bf-4dc7-f22d-8aa59dffba93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.81064284"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec.similarity('dog', 'puppy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYtBf4mfBPsj",
        "outputId": "970864df-b91b-4825-be9a-5b9de2757280"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cats', 0.8099379539489746),\n",
              " ('dog', 0.7609456777572632),\n",
              " ('kitten', 0.7464985251426697),\n",
              " ('feline', 0.7326233983039856),\n",
              " ('beagle', 0.7150583267211914),\n",
              " ('puppy', 0.7075453996658325),\n",
              " ('pup', 0.6934291124343872),\n",
              " ('pet', 0.6891531348228455),\n",
              " ('felines', 0.6755931377410889),\n",
              " ('chihuahua', 0.6709762215614319)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec.most_similar('cat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xixp7uGyEMTH"
      },
      "source": [
        "\n",
        "(king - man) + woman = queen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sms3zhUKBRVH",
        "outputId": "d376d2be-86ea-4a5e-faa2-5237ac688a04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('queen', 0.7118192911148071),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951),\n",
              " ('crown_prince', 0.5499460697174072),\n",
              " ('prince', 0.5377321243286133),\n",
              " ('kings', 0.5236844420433044),\n",
              " ('Queen_Consort', 0.5235945582389832),\n",
              " ('queens', 0.518113374710083),\n",
              " ('sultan', 0.5098593235015869),\n",
              " ('monarchy', 0.5087411999702454)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec.most_similar(positive=['woman', 'king'], negative=['man'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jogN9NS1VSHg",
        "outputId": "a4f8c6a5-bd6a-44dc-8656-bf7531e268d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Officer', 0.5694271326065063),\n",
              " ('officers', 0.538264274597168),\n",
              " ('offi_cer', 0.5283650159835815),\n",
              " ('chief', 0.48523107171058655),\n",
              " ('deputy', 0.47100305557250977),\n",
              " ('patrolwoman', 0.4685642719268799),\n",
              " ('policewoman', 0.46202757954597473),\n",
              " ('vice_president', 0.461116224527359),\n",
              " ('supervisor', 0.4552857577800751),\n",
              " ('oficer', 0.4532422721385956)]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec.most_similar(['woman', 'officer'], negative = ['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using word2vec embeddings"
      ],
      "metadata": {
        "id": "9ysjc3ZsXNxE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "954luqj1ai_r"
      },
      "source": [
        "Using the word2vec embeddings from Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAXlt86TUCWc",
        "outputId": "048a0a97-6990-44dc-896d-ef86b534b419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kEMTQSnUwis",
        "outputId": "91959983-9ac8-4c1c-f2de-cf711c7c39ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus  import twitter_samples\n",
        "\n",
        "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "print(len(pos_tweets))\n",
        "\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "print(len(neg_tweets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jLZO6yXUy7T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pos_df = pd.DataFrame(pos_tweets, columns = ['tweet'])\n",
        "pos_df['label'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NDhKlXNU0Hk"
      },
      "outputs": [],
      "source": [
        "neg_df = pd.DataFrame(neg_tweets, columns = ['tweet'])\n",
        "neg_df['label'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgQua4ukU1Jl"
      },
      "outputs": [],
      "source": [
        "data_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
        "# data_df = data_df[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGKhcf-QU3Gy",
        "outputId": "e5fb2d15-5397-4e7f-ce58-5ec3c0f7d103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  tweet  label\n",
            "6745                 @sweetbabecake yea i guess so :(((      0\n",
            "8859  I fucking hate when I wake up like at this tim...      0\n",
            "644         .@sajidislam honored to have you here ! :-)      1\n",
            "6013                    o otp :( http://t.co/EVislmNp5V      0\n",
            "2283  @effinoreos HAPPY 15th BIRTHDAY VIANEY!!! (a.k...      1\n",
            "...                                                 ...    ...\n",
            "4840  @scousebabe888 Nice Holiday Honey!!!!!!!!!!!!!...      1\n",
            "3976  @planetjedward GoodMorning ! What's coming nex...      1\n",
            "7151  I feel like I'm a weird person for shipping Be...      0\n",
            "6902  I met a new kinds of people, new classmate, ne...      0\n",
            "5326        @hamzaabasiali exactly but unfortunately :(      0\n",
            "\n",
            "[8000 rows x 2 columns]\n",
            "                                                  tweet  label\n",
            "5648  @jenxmish @wittykrushnic you are the only thin...      0\n",
            "7425                                   Omg no Amber :((      0\n",
            "255   @AvinPera  follow @jnlazts &amp; http://t.co/R...      1\n",
            "305   Hi BAM ! @BarsAndMelody \\nCan you follow my be...      1\n",
            "1427              @TheMattEspinosa you make me happy :)      1\n",
            "...                                                 ...    ...\n",
            "6279  @ellierowexo in this weather, are you mad? &am...      0\n",
            "7960                                  I WANT üçúüçúüçúüçúüçú :(((      0\n",
            "8655  idk where I went wrong I used to be so cute :(...      0\n",
            "5276                   @ABeezyGMT says the Man U fan :(      0\n",
            "5329                                     Aww too bad :(      0\n",
            "\n",
            "[2000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(data_df, test_size=0.2, shuffle = True)\n",
        "print(train_df)\n",
        "print(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UdnpkcGXmZv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "def compute_embeddings(df):\n",
        "    train_emb = []\n",
        "    for i, row in tqdm.tqdm(df.iterrows(), total = len(df.index)):\n",
        "        words = row['tweet'].split(' ')\n",
        "        words = filter(lambda x: x in word2vec.vocab, words)\n",
        "        text_emb = [word2vec[word] for word in words]\n",
        "\n",
        "        if len(text_emb) == 0:\n",
        "            train_emb.append(np.zeros(300))\n",
        "            continue\n",
        "\n",
        "        doc_embedding = np.mean(text_emb, axis = 0)\n",
        "        train_emb.append(doc_embedding)\n",
        "    return np.array(train_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o65sQbtMS66a",
        "outputId": "263af0e8-f602-460a-b616-360d05da8de8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:01<00:00, 4282.18it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 4114.12it/s]\n"
          ]
        }
      ],
      "source": [
        "X_train_emb = compute_embeddings(train_df)\n",
        "y_train = train_df['label']\n",
        "\n",
        "X_test_emb = compute_embeddings(test_df)\n",
        "y_test = test_df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcflA2mDU79N",
        "outputId": "fa9e364b-6bf1-457d-9645-4cf085e3d69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SVC(verbose=2)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(verbose = 2)\n",
        "svm.fit(X_train_emb, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7o_5xGfYJxG",
        "outputId": "dfc5f2ff-d3af-489b-ac37-f36ebf57f13b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.8815\n",
            "Precision 0.9507803121248499\n",
            "F1 score 0.869851729818781\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "\n",
        "y_test_pred = svm.predict(X_test_emb)\n",
        "\n",
        "print('Accuracy', accuracy_score(y_test, y_test_pred))\n",
        "print('Precision',precision_score(y_test, y_test_pred))\n",
        "print('F1 score',f1_score(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiHaJDLOgYnp"
      },
      "source": [
        "# Global Vectors (GloVe)\n",
        "\n",
        "While Word2Vec is based only on local statistics (the occurence of words at\n",
        "a single-sentence level) [GloVe](https://nlp.stanford.edu/projects/glove/) incorporates global statistics methods. This makes it better suited for smaller datasets, as it does not need as much training data.\n",
        "\n",
        "The model counts all \"word1 word2 ...\" pairs (for a context window of x we consider words that have at most distance x between them) and keeps the information in a co-occurrence matrix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ7WU2agz4b2"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?export=view&id=1pnX1lPdQItUauHp9W8xJlx8q2lgTe4cJ' width=500></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Y4Noij2Egg"
      },
      "source": [
        "Afterwards, it computes the probability that a word will be closer to another one based on this matrix:\n",
        "$$P(j | i) = \\frac{X_{ij}}{X_i}$$\n",
        "where:\n",
        "$$P(j | i) = the\\ probability\\ of\\ word\\ j\\ given\\ i$$\n",
        "$$X_{ij} = how\\ many\\ times\\ word\\ j\\ appears\\ in\\ the\\ context\\ of\\ i$$\n",
        "$$X_i = \\sum_k X_{ik} = sum\\ of\\ how\\ many\\ times\\ words\\ appear\\ in\\ the\\ context\\ of\\ i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg2yYtcC_TIJ"
      },
      "source": [
        "Based on this we should be able to infer relations between words:\n",
        "\n",
        "<center><img src='https://nlp.stanford.edu/projects/glove/images/table.png' width=500></center>\n",
        "\n",
        "Notice how _solid_ is related to _ice_ but not _steam_, while _gas_ is related to _steam_ but not _ice_ (very large vs. very small conditional values). _Water_ and _fashion_ on the other hand are either highly related to both or completely unrelated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW_-byU0Ad53"
      },
      "source": [
        "Some more computation will bring us to the regression model that is now used for this model. If you want to learn more you can check [the paper](https://aclanthology.org/D14-1162.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFdqo-FRQ7gP"
      },
      "source": [
        "## Using GloVe\n",
        "\n",
        "We can load a pretrained GloVe model using the gensim library (or other resources):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mY_dP3FQ6_n",
        "outputId": "7db963e5-d07a-426a-a131-2ffbfdc9e46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"glove-twitter-100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUKW1ADEVX5d"
      },
      "source": [
        "And use it to compute the word embeddings (or do all other similarity functions that we saw for Word2Vec):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6ZEmJuKR9S9",
        "outputId": "e937c2a9-78d0-46aa-ea3a-245a9f827a21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.43887 ,  0.32601 , -0.28524 , -0.08248 ,  0.43643 ,  0.75065 ,\n",
              "        0.093945, -0.72626 ,  0.32297 , -0.37128 , -0.23306 ,  0.35499 ,\n",
              "       -3.1764  ,  0.015004,  0.69725 , -0.15256 ,  0.025449, -0.058944,\n",
              "        0.20002 , -0.61298 , -0.79661 ,  0.53051 ,  0.64765 ,  0.90153 ,\n",
              "       -0.27407 ,  0.52871 ,  0.39344 ,  0.56076 ,  0.31942 ,  0.83347 ,\n",
              "       -0.53268 , -1.0166  , -0.25328 , -0.17347 ,  0.68794 ,  0.25902 ,\n",
              "        0.42864 ,  0.3844  , -0.071415, -0.026013, -0.42733 ,  0.58874 ,\n",
              "       -0.30061 , -0.18357 ,  0.21158 , -0.72648 , -0.48477 ,  0.43527 ,\n",
              "       -0.37412 , -0.48493 ,  0.26264 ,  0.21684 , -0.8822  ,  0.57925 ,\n",
              "       -0.54    ,  0.7147  , -0.33133 , -0.44715 , -0.40713 , -0.014364,\n",
              "       -0.083808,  0.45569 , -0.094374,  0.56057 ,  0.65446 , -0.45768 ,\n",
              "        0.2522  ,  0.34328 , -0.061001, -0.4899  ,  0.3342  ,  0.41277 ,\n",
              "       -0.55403 ,  0.30807 ,  0.22867 , -0.53921 ,  0.16439 ,  0.021561,\n",
              "        0.15131 , -0.70287 ,  1.4152  ,  0.83387 ,  0.44385 , -0.042976,\n",
              "        0.069162, -0.74432 , -0.032278, -0.6221  ,  0.20007 ,  0.15834 ,\n",
              "       -0.53907 , -0.31442 ,  0.60969 , -0.32378 ,  0.1676  , -0.94943 ,\n",
              "        0.52916 ,  0.035842, -0.041395, -0.56533 ], dtype=float32)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model['system']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc22faRVaCpg"
      },
      "source": [
        "Or you can train your own model from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG2UypDfaTjG"
      },
      "outputs": [],
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus()\n",
        "corpus.fit(common_texts, window=4)\n",
        "\n",
        "glove = Glove(no_components=4, learning_rate=0.1)\n",
        "glove.fit(corpus.matrix, epochs=10, no_threads=8, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove.model.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PI1K_gFayXI"
      },
      "source": [
        "# FastText\n",
        "\n",
        "The last embedding technique that we will talk about is FastText. With a really nice documentation, FastText also uses Skip-Gram and CBoW (like Word2Vec), but instead of learning words as a whole, it splits them in sequences of characters. This helps the model generalize better, especially with rare words, as it learns prefixes and suffixes along with other short sequences that convey information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Ui6YNbeAsX"
      },
      "source": [
        "If we choose to split the word _artificial_ in n-grams of size 3 and padding 1, the representation will be: <_ar_, _art_, _rti_, _tif_, _ifi_, _fic_, _ici_, _ial_, _al_>. And then we continue similar as with word2vec. The full explanation is in [the paper](https://aclanthology.org/E17-2068.pdf) and code snippets are in the [documentation](https://fasttext.cc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55UuO7SZRcJs"
      },
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a dimensionality reduction algorithm -- meaning that we can use it to visualise our data in 2D or 3D. Here is an example of how you can use it to see the distance between embeddings in 2D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSYbiUpQMaM-"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "text = ['system', 'graph', 'trees', 'user']\n",
        "embeddings = [model[word] for word in text]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(embeddings)\n",
        "vectors_2d = pca.transform(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can train it the same way we would a normal ML model, and visualize the results using, for example, a plotting library like matplotlib:"
      ],
      "metadata": {
        "id": "K3OgyZ5CB4Wh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxhT5BnPQZXI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [v[0] for v in vectors_2d]\n",
        "y = [v[1] for v in vectors_2d]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y)\n",
        "\n",
        "for i, txt in enumerate(text):\n",
        "    ax.annotate(txt, (x[i], y[i]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2pDbrSpph22"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1. Play around with the word2vec model and see if there are any interesting or counterintuitive similarity results using  ```word2vec.similarity``` and ```word2vec.most_similar```.\n",
        "\n",
        "2. Use other embeddings (glove, fasttext) to encode the data from the sentiment analysis task and train the classification model.\n",
        "\n",
        "3. Write your own implementation for Bag of Words from scratch. You should be able to set whether the representation will be binary or frequency-based.\n",
        "\n",
        "4. Implement your own TfIdf from scratch. You can use as many helper functions as you want.\n",
        "\n",
        "5. Create the (context, target) pairs and train a neural network for either skip-gram or continuous bag of words. You should quantify each word with a unique id and use padding at the beginning and end of the text for training on the marginal terms.\n",
        "\n",
        "6. Visualise the distance between a few words in 2D using PCA (or another dimensionality reduction technique)\n",
        "\n",
        "7. Compare these embeddings using any means (e.g.: train time, most similar word to X, distances in a 2D space, accuracy with a SVM etc.). Also compare the library versions with your own implementations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9pUgsa1ph22"
      },
      "source": [
        "Notebook adapted from https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n",
        "\n",
        "Further Reading\n",
        "\n",
        "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf)\n",
        "- [Debiaswe: try to make word embeddings less sexist](https://github.com/tolga-b/debiaswe)\n",
        "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "- [Fasttext Word vectors for 157 languages](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
        "- [Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)"
      ]
    }
  ]
}