{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gw3eZLKkU-fP",
        "UQMO4b6u007D",
        "izNSmtGue7gD",
        "k---xJS96EJu",
        "91W7wg7b6HY9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Written language contains a lot of elements that do not necessarily convey relevant information for the task at hand. Declination, punctuation, numbers etc. sometimes are better left outside the scope when modeling a text. Depending on the task we have to perform, we can choose one or more ways to clean the text:\n",
        "- Tokenize text\n",
        "- Convert to lowercase letters\n",
        "- Remove digits / numbers (or turn them into words)\n",
        "- Remove links\n",
        "- Remove emoticons ( :) :D) and emojis (ğŸ’™ ğŸ±)\n",
        "- Remove punctuation\n",
        "- Remove stopwords\n",
        "- Stem / Lemmatize words\n"
      ],
      "metadata": {
        "id": "U3sSizRnguC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegEx\n",
        "\n",
        "A RegEx (_Regular Expression_) represents a match pattern for a sequence of characters. It can be used to either simply identify a substring in a string, replace it or split around it.\n",
        "\n",
        "You can check how a regex matches a text using this link https://pythex.org/."
      ],
      "metadata": {
        "id": "gw3eZLKkU-fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "txt = \"The rain   in Spain stays mainly   in the plain\"\n",
        "x = re.search(\"Spain\", txt)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY3TpT-V5QX3",
        "outputId": "08b581d7-4668-464c-bea7-ab5ffda11777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(14, 19), match='Spain'>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Searching for a string returns nothing if no match is found, otherwise it returns an object with the exact string match an the position. The exact string match becomes relevant when we use other symbols for pattern matching, for example:\n",
        "- [.] - any character\n",
        "- [\\+] - multiple apparitions of the previous character\n",
        "- [\\*] - an indefinite number of apparitions of the previous character, including 0"
      ],
      "metadata": {
        "id": "6XX0_VEtWJAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = re.split(\" +.\", txt)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37jcxol7D7yu",
        "outputId": "95b6de60-e634-4cc0-c267-ef854f494185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'ain', 'n', 'pain', 'tays', 'ainly', 'n', 'he', 'lain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other special characters:\n",
        "- \\d - digits\n",
        "- \\D - not digits\n",
        "- \\s - space\n",
        "- \\w - lowercase characters, uppercase characters, the character \"_\"\n",
        "- [] - a set of characters written inside. It can include intervals\n",
        "\n",
        "The full library can be found here https://docs.python.org/3/library/re.html."
      ],
      "metadata": {
        "id": "7c-ppOwgEszZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use a regex to identify all words that include the sequence \"ai\":\n"
      ],
      "metadata": {
        "id": "iZBBAPXQtMAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = re.findall(\"\\w*ai\\w\", txt)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxme5CAus_uJ",
        "outputId": "95945a8f-db88-4895-90c1-f2d55e8b850d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rain', 'Spain', 'main', 'plain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encodings\n",
        "\n",
        "Most non-english languages include characters that are not a part of the ASCII table. The html standard at the moment is 'utf-8' (which extends the ASCII character set), while operating systems usually work on 'utf-16'. This doesn't mean that we can't encounter files encoded differently. A simple solution for identifying the right encoding is using the [chardet](https://pypi.org/project/chardet/) library:"
      ],
      "metadata": {
        "id": "UQMO4b6u007D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chardet"
      ],
      "metadata": {
        "id": "9L2JCNkmIvkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.yahoo.co.jp/\"\n",
        "html = requests.get(url)\n",
        "html.content"
      ],
      "metadata": {
        "id": "H3q8m3YmI1Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(html.content, 'html.parser')\n",
        "text = soup.get_text()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "BQoDBzbE-j9a",
        "outputId": "4b4e5513-c1e6-4919-ce5d-728bb3c4ef2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yahoo! JAPANYahoo! JAPANãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚«ãƒ¼ãƒ‰ãƒ¡ãƒ¼ãƒ«ãƒˆãƒ©ãƒ™ãƒ«ãƒ¤ãƒ•ã‚ªã‚¯!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ç½å®³æƒ…å ±åœŸç ‚ç½å®³æ±äº¬éƒ½å¤§é›¨ã«ã‚ˆã‚Šè­¦æˆ’ãƒ¬ãƒ™ãƒ«ãŒé«˜ã¾ã£ã¦ã„ã‚‹åœ°åŸŸãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°æ¤œç´¢ã‚¦ã‚§ãƒ–ç”»åƒå‹•ç”»çŸ¥æµè¢‹åœ°å›³ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¸€è¦§Yahoo!æ¤œç´¢æ¤œç´¢JavaScriptã®è¨­å®šã«ã¤ã„ã¦JavaScriptãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚ã™ã¹ã¦ã®æ©Ÿèƒ½ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã«ã¯ã€æœ‰åŠ¹ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚è©³ã—ãã¯ã€ŒJavaScriptã®è¨­å®šæ–¹æ³•ã€ã‚’ã”è¦§ãã ã•ã„ã€‚æ¨å¥¨ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼ã«ã¤ã„ã¦Yahoo! JAPANãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®æ©Ÿèƒ½ã‚’æ­£ã—ãã”åˆ©ç”¨ã„ãŸã ãã«ã¯ã€ä¸‹è¨˜ã®ç’°å¢ƒãŒå¿…è¦ã§ã™ã€‚ãƒ‘ã‚½ã‚³ãƒ³ã§ã”åˆ©ç”¨ã®ãŠå®¢æ§˜Windowsï¼šEdge æœ€æ–°ç‰ˆ / Chrome æœ€æ–°ç‰ˆ / Firefox æœ€æ–°ç‰ˆ\\u3000macOSï¼šSafari 15.0ä»¥ä¸Šã‚¿ãƒ–ãƒ¬ãƒƒãƒˆã§ã”åˆ©ç”¨ã®ãŠå®¢æ§˜iOS 15.0ä»¥é™ã€ã¾ãŸã¯ã€Android5.0ä»¥é™ã®OSã«æ¨™æº–æ­è¼‰ã•ã‚ŒãŸãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼â€»æ—¥æœ¬å›½å†…ç‰ˆã¨ã—ã¦ç™ºå£²ã•ã‚Œã¦ã„ã‚‹ç«¯æœ«ã§ã”åˆ©ç”¨ãã ã•ã„ã€‚ãŠçŸ¥ã‚‰ã›ãƒãƒ­ã‚¦ã‚£ãƒ³æ°—åˆ†ã‚’ç››ã‚Šä¸Šã’ã‚ˆã†\\u3000ä»®è£…è¡£è£…ã‚„æ‰‹ä½œã‚Šå°ç‰©ã€Œåæ¢åµã‚³ãƒŠãƒ³ã€ã€Œã‚¾ãƒ³100ã€ã»ã‹117å†Šç„¡æ–™ã‚½ãƒªãƒ†ã‚£ã‚¢ãªã©ã‚²ãƒ¼ãƒ 500ç¨®ä»¥ä¸Šä¸»ãªã‚µãƒ¼ãƒ“ã‚¹ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ãƒ¤ãƒ•ã‚ªã‚¯!PayPayãƒ•ãƒªãƒZOZOTOWNLOHACOãƒˆãƒ©ãƒ™ãƒ«ä¸€ä¼‘.comä¸€ä¼‘.comãƒ¬ã‚¹ãƒˆãƒ©ãƒ³PayPayã‚°ãƒ«ãƒ¡å‡ºå‰é¤¨ã‚µãƒ¼ãƒ“ã‚¹ä¸€è¦§ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸»è¦ ãƒ‹ãƒ¥ãƒ¼ã‚¹10/18(æ°´) 17:18æ›´æ–°å…¨éŠ€ã‚·ã‚¹ãƒ†ãƒ éšœå®³ è£œå„Ÿã‚’ç™ºè¡¨NEWå¤–å‹™çœ ã‚¤ã‚¹ãƒ©ã‚¨ãƒ«åŒ—éƒ¨ã«é€€é¿å‹§å‘ŠNEWæŠ€èƒ½å®Ÿç¿’ã®å»ƒæ­¢ã¸æ–°åˆ¶åº¦æ¡ˆã‚’æå‡ºå‰£é“éƒ¨å“¡æ­»äº¡ è¿‘ç•¿å¤§å­¦ãŒè¬ç½ªå¥³å…æ­»äº¡ ç”·ã¯é‹è»¢ã«ä¸å®‰ã‚ã£ãŸã‹å­ãŒ4å¹´é–“ä¸ç™»æ ¡ å¸¸è­˜ã‚’æ¨ã¦ãŸæ¯NEWU22æ—¥æœ¬ä»£è¡¨ãŒ4å¤±ç‚¹ ç±³å›½ã«å®Œæ•—NEWè¦‹å–ã‚Šå›³ç››å±±ã«çµå©šå ±å‘Š ã›ã„ã‚„æ¶™é•·æ—…ã®ç¾½ä¼‘ã‚10/18(æ°´) 14:07æ¯æ—¥æ–°èã‚‚ã£ã¨è¦‹ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚¹ä¸€è¦§LINEãƒ¤ãƒ•ãƒ¼æ ªå¼ä¼šç¤¾ä¼šç¤¾æ¦‚è¦ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£åºƒå‘Šæ²è¼‰ã«ã¤ã„ã¦æ¡ç”¨æƒ…å ±åˆ©ç”¨è¦ç´„å…è²¬äº‹é …ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚»ãƒ³ã‚¿ãƒ¼æŠ•è³‡å®¶æƒ…å ±Â© LY Corporation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "\n",
        "chardet.detect(html.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsVmfhBCQ1F6",
        "outputId": "91baba2a-9e8b-443c-e21a-a1a3eb1859f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might also need to access these characters in their closest ASCII form. In these cases we can use [unidecode](https://pypi.org/project/Unidecode/):"
      ],
      "metadata": {
        "id": "vmJWRG3_9RFN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5BIgaTjvwwt"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "\n",
        "unidecode(soup.get_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "zbEiPrXqAUbw",
        "outputId": "97ccdb0f-7563-4f89-9bd2-8d0a5e14749c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yahoo! JAPANYahoo! JAPANpuremiamuka-dome-rutoraberuyahuoku!shiyotsupinguZai Hai Qing Bao Tu Sha Zai Hai Dong Jing Du Da Yu niyoriJing Jie reberugaGao matsuteiruDi Yu gaarimasu. Xiang Xi Jian Suo uebuHua Xiang Dong Hua Zhi Hui Dai Di Tu riarutaimuYi Lan Yahoo!Jian Suo Jian Suo JavaScriptnoShe Ding nitsuiteJavaScriptgaWu Xiao ninatsuteimasu. subetenoJi Neng woLi Yong surutameniha, You Xiao niShe Ding shitekudasai. Xiang shikuha[JavaScriptnoShe Ding Fang Fa ] wogoLan kudasai. Tui Jiang burauza-nitsuiteYahoo! JAPANtotsupupe-zinoJi Neng woZheng shikugoLi Yong itadakuniha, Xia Ji noHuan Jing gaBi Yao desu. pasokondegoLi Yong nooKe Yang Windows:Edge Zui Xin Ban  / Chrome Zui Xin Ban  / Firefox Zui Xin Ban  macOS:Safari 15.0Yi Shang taburetsutodegoLi Yong nooKe Yang iOS 15.0Yi Jiang , mataha, Android5.0Yi Jiang noOSniBiao Zhun Da Zai saretaburauza-*Ri Ben Guo Nei Ban toshiteFa Mai sareteiruDuan Mo degoLi Yong kudasai. oZhi raseharouinQi Fen woSheng riShang geyou Jia Zhuang Yi Zhuang yaShou Zuo riXiao Wu [Ming Tan Zhen konan] [zon100] hoka117Ce Wu Liao soriteianadoge-mu500Zhong Yi Shang Zhu nasa-bisushiyotsupinguyahuoku!PayPayhurimaZOZOTOWNLOHACOtoraberuYi Xiu .comYi Xiu .comresutoranPayPaygurumeChu Qian Guan sa-bisuYi Lan niyu-suZhu Yao  niyu-su10/18(Shui ) 17:18Geng Xin Quan Yin shisutemuZhang Hai  Bu Chang woFa Biao NEWWai Wu Sheng  isuraeruBei Bu niTui Bi Quan Gao NEWJi Neng Shi Xi noFei Zhi heXin Zhi Du An woTi Chu Jian Dao Bu Yuan Si Wang  Jin Ji Da Xue gaXie Zui Nu Er Si Wang  Nan haYun Zhuan niBu An atsutakaZi ga4Nian Jian Bu Deng Xiao  Chang Shi woShe tetaMu NEWU22Ri Ben Dai Biao ga4Shi Dian  Mi Guo niWan Bai NEWJian Qu riTu Sheng Shan niJie Hun Bao Gao  seiyaLei Chang Lu noYu Xiu me10/18(Shui ) 14:07Mei Ri Xin Wen motsutoJian rutopitsukusuYi Lan LINEyahu-Zhu Shi Hui She Hui She Gai Yao sasutenabiriteiGuang Gao Jie Zai nitsuiteCai Yong Qing Bao Li Yong Gui Yue Mian Ze Shi Xiang medeiasute-tomentopuraibashi-porishi-puraibashi-senta-Tou Zi Jia Qing Bao (c) LY Corporation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unidecode('LeneÅŸul mai mult aleargÄƒ, scumpul mai mult pÄƒgubeÅŸte')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vC2r5I-jAxg9",
        "outputId": "ee1292f7-4622-4b21-d17e-80adb50da185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lenesul mai mult alearga, scumpul mai mult pagubeste'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "Tokenization is the process of splitting a text into _tokens_. Keep in mind that tokens are not necessarily words or sentences, but rather any sequence of characters split by a certain rule."
      ],
      "metadata": {
        "id": "cbpK2r2z6mgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the following exercises we will use a corpus from [NLTK](https://www.nltk.org/) for sentiment analysis."
      ],
      "metadata": {
        "id": "y9ab1d6c0vAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "tweets"
      ],
      "metadata": {
        "id": "xFT9RrgQ0vAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "print(tweets[6])\n",
        "print(sent_tokenize(tweets[6]))\n",
        "print(word_tokenize(tweets[6]))"
      ],
      "metadata": {
        "id": "2wMg2p2bFSkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4c5482-f005-432c-bd81-7bb1c5220831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
            "[\"We don't like to keep our lovely customers waiting for long!\", 'We hope you enjoy!', 'Happy Friday!', '- LWWF :) https://t.co/smyYriipxI']\n",
            "['We', 'do', \"n't\", 'like', 'to', 'keep', 'our', 'lovely', 'customers', 'waiting', 'for', 'long', '!', 'We', 'hope', 'you', 'enjoy', '!', 'Happy', 'Friday', '!', '-', 'LWWF', ':', ')', 'https', ':', '//t.co/smyYriipxI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how nltk splits the word \"don't\". Some tokenizers choose to keep it intact. This is just an example of a tokenization choice we must make."
      ],
      "metadata": {
        "id": "X_R5Pa8Q2Rxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emoticons & emojis\n",
        "\n",
        "Tokenization is usually based on spaces and punctuation, which means that it cannot deal with emoticons. One option is to create your own regex that can identify the symbols and replace them with the corresponding emotion.\n",
        "\n",
        "A small example:"
      ],
      "metadata": {
        "id": "izNSmtGue7gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoticons = {\n",
        "    \"happy\": r\":[\\)|D+]\",\n",
        "    \"laugh\": r\":\\)\\)+\",\n",
        "    \"sad\": r\":\\(+\"\n",
        "}"
      ],
      "metadata": {
        "id": "JVKGAPTzKrwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For emoticons we can use the [emoji](https://pypi.org/project/emoji/) library:"
      ],
      "metadata": {
        "id": "dL_3jA0j-HrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2jGoaJnQUGD",
        "outputId": "435cad4b-5ad3-4e66-8cdb-bf8d562b9f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/358.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m358.4/358.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "\n",
        "print(tweets[24])\n",
        "emoji.demojize(tweets[24])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MgVVPDioOeFx",
        "outputId": "0bd32ca5-e94f-447f-fec2-ba56484c5a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’…ğŸ½ğŸ’‹ - :)))) haven't seen you in years\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\":nail_polish_medium_skin_tone::kiss_mark: - :)))) haven't seen you in years\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization vs. Stemming\n",
        "\n",
        "Most times the exact format in which a word appears in a text is not relevant, but rather the information it conveys and the frequence it has in a text. This is why you would want to reduce it to its simplest form, whatever that is.\n",
        "\n",
        "For lemmatization the simplest form means the one found in a dictionary and is called a _lemma_. Stemming uses regex to remove prefixes and sufixes, which means that the _stem_ might not be a real word, or it might mean something completely different.\n",
        "\n",
        "We use stemming when we want a fast answer or when dealing with many words that are not written correctly (as on social media)."
      ],
      "metadata": {
        "id": "k---xJS96EJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![1_HLQgkMt5-g5WO5VpNuTl_g.jpeg](https://miro.medium.com/max/564/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg)"
      ],
      "metadata": {
        "id": "SYFj69rC1T3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of how you can use a lemmatizer or a stemmer for english:"
      ],
      "metadata": {
        "id": "rzKROdKPfIvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "uaC3VE7GfxvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"leaves\""
      ],
      "metadata": {
        "id": "Z9jeWhD321z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(f\"{word} :\", lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrEn9Ytl1ycZ",
        "outputId": "11133c52-e807-4912-b407-fa4da37cee34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaves : leaf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(f\"{word} :\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjH_MhOY2poF",
        "outputId": "22639455-61a8-49a1-9b3b-38922c537be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaves : leav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords\n",
        "\n",
        "Stopwords are the most used words in a language. While they have syntactic and morphological value, stopwords do not have much semantic value. Most stopwords are pronouns (\"we\"), prepositions (\"for\") or conjunctions (\"and\"), but there are also verbs (\"is\") or numerals (\"two\").\n",
        "\n",
        "[Fun fact](https://www.youtube.com/watch?v=fCn8zs912OE)"
      ],
      "metadata": {
        "id": "91W7wg7b6HY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZS8k63_rXB0",
        "outputId": "5aae5792-c91d-436c-ed3f-495211f3c4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "print(stop_words_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4gju2Q9zUft",
        "outputId": "ce4659c7-1c40-4bee-eca0-3f203443d37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'through', 'on', 'each', 'out', \"don't\", 'me', 'o', 'in', \"won't\", 'before', \"that'll\", 'm', 'ourselves', \"shouldn't\", 'nor', 'it', \"you'll\", 'has', 'all', 'some', 're', 'to', 'did', 'with', \"wasn't\", 'this', \"isn't\", 'during', 'further', \"haven't\", 'while', 'the', 'over', 'couldn', 'same', 'and', 'have', 'here', \"she's\", 'between', 'aren', \"hasn't\", 'had', 'by', 'other', 'because', \"weren't\", 'than', 't', 'haven', 'until', 'but', 'y', \"couldn't\", 'why', 'do', 've', \"needn't\", 'yourselves', 'having', 's', 'is', 'just', 'isn', 'theirs', 'their', 'myself', 'whom', 'him', 'where', 'yours', 'hasn', 'doing', 'being', \"you'd\", 'they', 'were', 'if', 'shan', 'wasn', 'as', \"should've\", 'll', \"mustn't\", 'off', 'we', 'own', 'under', 'too', \"aren't\", 'shouldn', 'i', 'ma', 'he', 'above', 'once', 'will', 'against', 'his', 'hadn', 'ain', \"didn't\", 'that', 'how', 'there', 'most', 'hers', 'itself', 'our', \"hadn't\", 'then', \"doesn't\", 'd', 'her', 'them', 'be', 'very', 'can', 'she', 'more', 'ours', \"you're\", 'you', 'of', 'your', 'don', 'doesn', 'mightn', 'below', \"wouldn't\", 'both', 'needn', 'should', \"it's\", 'or', 'for', 'won', 'which', 'been', 'so', 'what', 'was', 'only', 'themselves', 'are', 'who', \"mightn't\", 'an', 'again', \"you've\", 'herself', 'few', 'himself', 'when', 'weren', 'its', 'a', 'down', 'not', \"shan't\", 'those', 'no', 'now', 'my', 'these', 'am', 'any', 'after', 'yourself', 'wouldn', 'from', 'about', 'mustn', 'such', 'didn', 'up', 'at', 'into', 'does'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK vs. SpaCy\n",
        "\n",
        "[NLTK](https://www.nltk.org/) and [SpaCy](https://spacy.io/) are some of the most common NLP libraries, since they implement most basic functionalities and a few other things. The main difference between them is that NLTK was build for researchers and scholars and gives you easier access to basic functions and customizations, while SpaCy was designed for app developers, for whom is not as important how things are done as long as the solution is good enough. So SpaCy is faster and sometimes better, but has a higher degree of abstractization and it will not always be very clear what happens behind the scenes."
      ],
      "metadata": {
        "id": "zvGoy2ehzcQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember the NLTK tokenization. SpaCy is usually not based on rules, but rather on pretrained models that are then fit to our examples. Notice the differences in how we use the tokenizer function and the design choices for token definition:"
      ],
      "metadata": {
        "id": "2O8wG8nPpEvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "tokens = tokenizer(tweets[6])\n",
        "for token in tokens:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWYujmHzbNs",
        "outputId": "f7080ee8-9847-4d50-c23f-0348d2e56d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We\n",
            "don't\n",
            "like\n",
            "to\n",
            "keep\n",
            "our\n",
            "lovely\n",
            "customers\n",
            "waiting\n",
            "for\n",
            "long!\n",
            "We\n",
            "hope\n",
            "you\n",
            "enjoy!\n",
            "Happy\n",
            "Friday!\n",
            "-\n",
            "LWWF\n",
            ":)\n",
            "https://t.co/smyYriipxI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy has a series of models finetuned on specific datasets, like the one shown below: *en\\_core\\_web\\_sm*. Notice the keywords in its name: it is trained on english data from the web and it has a small size. You can find all models [here](https://spacy.io/models)."
      ],
      "metadata": {
        "id": "MNvOZZf1p4TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(f\"{word} :\", nlp(word)[0].lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCsmtAo6xuof",
        "outputId": "6c34dfcc-d73b-43c0-ffaf-9c1d9e379938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaves : leave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For extracting the lemma we apply our model on a text and extract the lemma for each specific word. SpaCy does not have a stemmer, but it does have a different set of stopwords depending on the chosen model:"
      ],
      "metadata": {
        "id": "r_N3WH5os400"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "\n",
        "print(stop_words_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KBg8j8PrfqQ",
        "outputId": "c0dd4e32-2cbb-4e99-f4dc-31b601e10868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'on', 'however', 'seemed', 'make', 'towards', 'upon', 'has', 'some', 'beside', 'everyone', 'with', 'using', 'nine', 'during', 'seeming', 'without', 'done', 'â€™ll', 'unless', 'six', 'side', 'but', 'either', 'do', 'just', 'used', 'â€˜m', 'mostly', 'whom', 'him', 'sometime', 'yours', 'front', 'empty', 'were', 'hundred', 'beyond', 'as', 'go', 'still', 'could', 'somehow', 'whenever', 'noone', 'really', 'hers', 'itself', \"'re\", 'her', 'top', 'two', 'both', 'various', 'whence', 'beforehand', 'anywhere', 'whither', 'should', 'or', 'been', 'was', 'only', 'whereafter', 'full', 'who', 'whose', 'seems', 'twenty', 'move', 'few', 'eleven', 'herself', 'when', 'become', 'hereby', 'â€˜re', 'hereupon', 'no', 'am', 'after', 'amount', 'at', 'onto', 'something', 're', 'everywhere', 'latter', 'each', 'in', 'many', 'all', 'to', 'bottom', 'anyhow', 'nâ€˜t', 'and', 'between', 'had', 'by', 'eight', 'neither', 'us', 'well', 'therefore', 'three', 'namely', 'is', 'thus', \"'ve\", 'their', 'myself', 'anyway', 'they', 'elsewhere', 'hence', 'own', 'must', 'nâ€™t', 'he', 'whereby', 'hereafter', 'against', 'may', 'back', 'behind', 'among', 'that', 'there', 'call', 'then', 'whoever', 'them', 'be', 'already', 'more', 'ours', 'you', 'together', 'wherein', 'thereby', 'along', 'thereafter', 'so', 'fifty', 'are', 'since', 'thru', 'therein', 'again', 'meanwhile', 'never', 'less', 'those', 'became', 'these', 'yourself', 'everything', 'say', 'into', 'get', 'through', 'give', 'quite', 'alone', 'out', 'take', 'regarding', 'before', 'rather', 'whereas', 'via', 'it', 'anything', 'becomes', 'â€˜ll', 'though', 'herein', 'while', 'over', 'same', 'have', 'else', 'per', 'because', 'â€™re', 'next', 'several', 'nothing', 'none', 'third', 'forty', 'whatever', 'former', 'made', 'why', 'cannot', 'whereupon', 'yourselves', 'least', 'throughout', 'being', 'â€™d', 'off', 'we', 'show', 'under', 'once', 'toward', \"'ll\", 'sometimes', 'his', 'part', 'others', 'also', 'how', 'almost', 'always', 'twelve', 'often', 'can', 'of', 'â€˜s', 'please', 'below', \"'d\", 'another', 'much', 'for', 'which', 'what', 'enough', 'indeed', 'sixty', 'wherever', 'himself', 'nevertheless', 'becoming', 'not', 'â€˜ve', 'keep', 'latterly', 'any', 'from', 'ca', 'such', 'up', 'does', 'besides', 'within', 'me', 'whole', 'ourselves', \"n't\", 'nor', 'would', 'thence', \"'s\", 'afterwards', 'did', 'first', 'this', 'yet', 'fifteen', 'further', \"'m\", 'â€˜d', 'the', 'formerly', 'one', 'here', 'might', 'otherwise', 'serious', 'other', 'than', 'ten', 'until', 'whether', 'â€™m', 'nowhere', 'where', 'doing', 'name', 'if', 'thereupon', 'â€™s', 'amongst', 'except', 'too', 'i', 'above', 'see', 'last', 'nobody', 'someone', 'will', 'every', 'somewhere', 'mine', 'most', 'our', 'â€™ve', 'around', 'very', 'she', 'even', 'across', 'seem', 'your', 'anyone', 'due', 'themselves', 'an', 'four', 'ever', 'moreover', 'a', 'down', 'its', 'now', 'although', 'my', 'five', 'put', 'about', 'perhaps'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "\n",
        "1. Write a function that splits a text into tokens using regex.\n",
        "Write a function that replaces all emoticons (not only the ones given as an example) and emojis in a text.\n",
        "2. Write a function that receives a text and returns its preprocessed version. 3. The function will convert all numbers into words using [num2words](https://pypi.org/project/num2words/), remove links, hashtags and mentions, eliminate punctuation, lowercase all letters, divide the text into tokens, remove stopwords and apply lemmatization or stemming.\n",
        "4. Analise the dataset. You can look at the mentioned preprocessing elements or other features (certain words, sentence length etc). What seems important?\n",
        "5. Based on the the previous analysis, write a preprocessing function that only removes the information you deem. You can generalize the original function to specify in the parameter list what changes you want to make when you call the function, you can make it a class that contains a series of functions and you pick your choice at initialization etc.\n",
        "6. Based on previous observations, write a function that adds the features you deem relevant to the dataset.\n",
        "6. Compare different preprocessing techniques / sets of features by training and testing on a simple model (eg: svm)."
      ],
      "metadata": {
        "id": "gDzC5KJbnWem"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gY4MRVrnM2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}