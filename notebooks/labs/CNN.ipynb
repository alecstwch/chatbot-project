{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "\n",
        "\n",
        "Neural networks (NN - Neural Networks) are a collection of chained functions that are applied to some input data. These functions are defined by parameters (weights and biases), stored through tensors.\n"
      ],
      "metadata": {
        "id": "ZahJWOQcnHg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input data is a matrix. For an image, the matrix will represent pixel values. For text, each line will probably represent a word. It is important to know that all lines must have the same length, so we will apply padding when necessary."
      ],
      "metadata": {
        "id": "I9vbErLX5_PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each arrow between neurons is a function. The output of that function is the value we find in the new neuron, which we can see as a \"How likely is it that this data that we look at has X feature?\". How do we combine multiple functions?"
      ],
      "metadata": {
        "id": "UdBnTCBS6e76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$N = \\sum_{i = 0}^n w_i * x_i + b$\n",
        "\n",
        "where:\n",
        "- $w_i$ is the weight for each node i\n",
        "- $x_i$ is the value in each node i\n",
        "- $b$ is the bias for the current layer"
      ],
      "metadata": {
        "id": "gIDHYaKtwxTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network (CNN)\n",
        "\n",
        "[A Convolution Visualiser](https://ezyang.github.io/convolution-visualizer/)\n",
        "\n",
        "Dense Neural Networks learn features in the positions in the training set, so it works for centered images, as symmetrical as possible. If it is trained on a picture of an animal on the left side of the image, it will probably learn to recognize the eyes, ears, and nose, but only as long as it is on that side. If we rotate the image horizontally, it will most likely not be recognized.\n",
        "\n",
        "This happens because the model learns the features in the position it is in.\n",
        "\n",
        "To solve this problem, we can use CNNs. A CNN learns what a feature in an image looks like, without linking it to a position. It creates an _output feature map_ for each feature and searches for it with a sliding window."
      ],
      "metadata": {
        "id": "Ymh3tu1Zn4Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNNs for text classification\n",
        "\n",
        "CNNs for text classification were first introduced in [Convolutional Neural Networks for Sentence Classification](https://aclanthology.org/D14-1181.pdf). This is the architecture that we will use in this lab.\n",
        "\n"
      ],
      "metadata": {
        "id": "YNzBsYHukEf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://richliao.github.io/images/YoonKim_ConvtextClassifier.png\">\n",
        "\n",
        "Given as input a text of $n$ words $w_{1}$, $w_{2}$, ..., $w_{n}$, we transform each word into a vector of size $d$, resulting in the vectors $w_{1}$, $w_{2}$, ..., $w_{n}$ belonging to $R^d$. The resulting $d$×$n$ matrix is then used as input for a convolutional layer that passes a sliding window over the text.\n",
        "\n",
        "For each window of length $l$:\n",
        "\n",
        "$u_{i}$ = [$w_{i}$, ..., $w_{i+l-1}$] $∈ R^{d×l}$, 0≤$i$≤$n-l$\n",
        "\n",
        "For each filter $f_{j} ∈ R^{d×l}$ we calculate <$u_{i}$, $f_{j}$> and obtain the matrix $ F ∈ R^{m×n}$ (if we have padded before applying the filter so that we keep the size $n$ of words), where $m$ is the number of filters. We apply max-pooling to the resulting $F$ matrix, then apply the activation function. Finally, we have a *fully connected* layer that produces the class distribution, from which the class with the highest probability results."
      ],
      "metadata": {
        "id": "4LmDoNVtPakL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images vs Text\n",
        "\n",
        "To understand why an approach using CNNs is suitable for text, we need to visualize our texts as a matrix.\n",
        "For the following example we will consider that the representation of a sentence was done at the word level.\n",
        "\n",
        "For example, for a sentence with a maximum length of 70 words and an embedding length of 300, we can create an array of numeric values of the form 70x300 to represent this sentence. Unlike images, where matrix elements are represented by pixel values, each line in the vector representation of the sentence is actually the representation of a word.\n",
        "\n",
        "For images, the convolution filter moves both vertically and horizontally, but for text, the filter only moves vertically, the convolutions are only 1D. A kernel of size (2, 300) that has a filter size of 2 only looks at 2 words at a time. We can therefore think of the size of the filters as a size of n-grams (bigrams, trigrams, etc.)."
      ],
      "metadata": {
        "id": "T3_PYPN8fajE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the IMDb movie reviews dataset: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "id": "4Z5b8qtPXLdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSKhMG6_U9Ch",
        "outputId": "1177bd91-cf01-4dd3-e6f9-2251450050b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.8/dist-packages (1.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from unidecode import unidecode\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3RBIBZPU2Qp",
        "outputId": "21e4f986-ceec-4fc6-b1d4-fe37fe45dc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDEgcB_E8uWL",
        "outputId": "bc9f9297-c359-497f-c2eb-d330f651864d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x7fa19e0ba970>)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1cush6tEdAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "187a1edb-ac93-4894-a40e-a72db8454b8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 review  sentiment\n",
              "0     My family and I normally do not watch local mo...          1\n",
              "1     Believe it or not, this was at one time the wo...          0\n",
              "2     After some internet surfing, I found the \"Home...          0\n",
              "3     One of the most unheralded great works of anim...          1\n",
              "4     It was the Sixties, and anyone with long hair ...          0\n",
              "...                                                 ...        ...\n",
              "9995  The film maybe goes a little far, but if you l...          1\n",
              "9996  This two-parter was excellent - the best since...          1\n",
              "9997  Shaggy & Scooby-Doo Get a Clue. It's like watc...          0\n",
              "9998  Todd Rohal is a mad genius. \"Knuckleface Jones...          1\n",
              "9999  Charlie Wilson's War, based on a true story, t...          1\n",
              "\n",
              "[10000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64fa6528-9edc-440a-95ce-1a3d01694b99\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My family and I normally do not watch local mo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Believe it or not, this was at one time the wo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After some internet surfing, I found the \"Home...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>One of the most unheralded great works of anim...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>The film maybe goes a little far, but if you l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>This two-parter was excellent - the best since...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>Shaggy &amp; Scooby-Doo Get a Clue. It's like watc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>Todd Rohal is a mad genius. \"Knuckleface Jones...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>Charlie Wilson's War, based on a true story, t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64fa6528-9edc-440a-95ce-1a3d01694b99')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-64fa6528-9edc-440a-95ce-1a3d01694b99 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-64fa6528-9edc-440a-95ce-1a3d01694b99');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "data = pd.read_csv('IMDB_Dataset.csv')\n",
        "data = data[:10000]\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into train and test."
      ],
      "metadata": {
        "id": "qaZmejBvY2jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(data, test_size=0.20, random_state = 42)\n",
        "\n",
        "print('Training set size', len(train_df))\n",
        "print('Testing set size', len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVFj4JHVY19U",
        "outputId": "ccdb8b9d-051e-49ec-d915-946c765d3203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size 8000\n",
            "Testing set size 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen in past labs, we cannot train a model directly on textual data, we must transform the data into vector numerical representations.\n",
        "\n",
        "For this, we have to go through 2 steps:\n",
        "\n",
        "- **Normalization**: for this example we will only tokenize the texts (splitting them into smaller subtexts)\n",
        "\n",
        "- **Vectorization**: representing in vector numerical format"
      ],
      "metadata": {
        "id": "4i3KQ4qbkcR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character-level vector representation"
      ],
      "metadata": {
        "id": "5VNrOeWin9YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
        "```\n",
        "\n",
        "In addition to the tokens present in our texts, we also add 2 special tokens: UNK (unknown word) and PAD.\n",
        "\n",
        "\n",
        "```\n",
        "Index assigned for every token: {0: 'UNK', 1: 'PAD', 2: 't', 3: 'm', 4: 'c', 5: 'h', 6: 'l', 7: 'w', 8: ' ', 9: 'a', 10: 'k', 11: 'e', 12: 'r', 13: 'u', 14: 'n', 15: 's', 16: 'd', 17: 'p', 18: 'o'}\n",
        "```\n",
        "\n",
        "The vector representation of the two texts using the corresponding index for each word:\n",
        "\n",
        "```\n",
        "'The mouse ran up the clock' = [18, 2, 9, 11, 15, 16, 12, 3, 9, 11, 13, 5, 8, 11, 12, 7, 11, 18, 2, 9, 11, 4, 6, 16, 4, 10]\n",
        "'The mouse ran down' = [18, 2, 9, 11, 15, 16, 12, 3, 9, 11, 13, 5, 8, 11, 14, 16, 17, 8]\n",
        "```\n",
        "\n",
        "We add padding values to the second vector to have a length equal to the first vector and we get:\n",
        "\n",
        "```\n",
        "[2, 16, 12, 6, 11, 13, 15, 5, 12, 6, 14, 7, 17, 6, 9, 13, 10, 17, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "```"
      ],
      "metadata": {
        "id": "fmzl7MpVECgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercises\n",
        "\n",
        "1. Create the dictionary of unique characters and their corresponding id for the train dataset. We will use 0 for unknown, 1 for padding and the numbers from 2... for the others. You can transform utf8 characters to their closest ASCII form to reduce the size of the vocabulary, but don't forget to add this transformation to the test set as well before predicting.\n",
        "2. Define a padding function. It should receive a text and a maximum length and\n",
        "  a. Truncate the text to the given length (if it's bigger)\n",
        "  b. Add the padding value to shorter texts, until they achieve the desired length\n",
        "3. Transform the train and test set into their vector representation -- replace each datapoint with a list of chars, replace these chars with their corresponding id, then pad each line to the desired length."
      ],
      "metadata": {
        "id": "cYF7L6eRQyBr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnGHk5MJQw3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An Architecture\n",
        "\n",
        "We will load our data sets into an object of the class [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)."
      ],
      "metadata": {
        "id": "aYAmW5bYWskw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = samples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, k):\n",
        "        \"\"\"Returns the kth example from the dataset\"\"\"\n",
        "        return self.samples[k], self.labels[k]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the size of the dataset\"\"\"\n",
        "        return len(self.samples)"
      ],
      "metadata": {
        "id": "nxJxJUYGXFmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the model architecture:"
      ],
      "metadata": {
        "id": "5z3wrGXaXKf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # We define an embedding layer with a vocabulary of size 1024\n",
        "        # and as output an embedding of size 100\n",
        "        # padding_idx is the index in the padding vocabulary (1, in our case)\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(1024, 100, padding_idx=1)\n",
        "\n",
        "        # Let's define a sequence of layers\n",
        "\n",
        "        # A dropour layer with a probability of 0.4\n",
        "        self.dropout = torch.nn.Dropout(p=0.4)\n",
        "\n",
        "        # A 1D Convolutional layer with 100 input channels, 128 output channels, kernel size = 3 and padding = 1\n",
        "        # 1D Batch Normalization Layer for 128 features\n",
        "        # ReLU activation\n",
        "        # 1D Maxpooling layer of size 2\n",
        "        conv1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm1d(num_features=128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # A 1D Convolutional layer with 100 input channels, 128 output channels, kernel size = 5 and padding = 1\n",
        "        # 1D Batch Normalization Layer for 128 features\n",
        "        # ReLU activation\n",
        "        # 1D Maxpooling layer of size 2\n",
        "        conv2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
        "            torch.nn.BatchNorm1d(num_features=128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # A 1D Convolutional layer with 100 input channels, 128 output channels, kernel size = 5 and padding = 1\n",
        "        # 1D Batch Normalization Layer for 128 features\n",
        "        # ReLU activation\n",
        "        # 1D Maxpooling layer of size 2\n",
        "        conv3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
        "            torch.nn.BatchNorm1d(num_features=128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # Global Average pooling layer which, in our case, is a 1D Avgerage Pooling layer\n",
        "        # with size 125 and stride 125\n",
        "        global_average = torch.nn.AvgPool1d(kernel_size=125, stride=125)\n",
        "\n",
        "        self.convolutions = torch.nn.Sequential(\n",
        "            conv1, conv2, conv3, global_average\n",
        "        )\n",
        "\n",
        "        # Flattening layer\n",
        "        flatten = torch.nn.Flatten()\n",
        "\n",
        "        # Linear layer with 128 input features and 2 outputs without activation function\n",
        "        linear = torch.nn.Linear(in_features=128, out_features=2)\n",
        "\n",
        "        self.classifier = torch.nn.Sequential(flatten, linear)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # we pass the input through the embedding layer\n",
        "        embeddings = self.embedding(input)\n",
        "\n",
        "        # we permute the input so that the first dimension is the number of channels\n",
        "        embeddings = embeddings.permute(0, 2, 1)\n",
        "\n",
        "        # we pass the input through the sequence of layers\n",
        "        output = self.dropout(embeddings)\n",
        "        output = self.convolutions(output)\n",
        "        output = self.classifier(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "mZbfT7b5XMqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\")\n",
        "# initializing the model\n",
        "model = Model().to(DEVICE)\n",
        "\n",
        "# Adam optimizer with learning rate = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Cross Entropy loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# training dataset and dataloader\n",
        "# test dataset and dataloader\n",
        "train_ds = Dataset(train_reviews_vectorized, train_labels)\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_ds = Dataset(test_reviews_vectorized, test_labels)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "EEAFYrkrXOR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "bVy9KScOVgCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = 0\n",
        "for epoch_n in range(5):\n",
        "    print(f\"Epoch #{epoch_n + 1}\")\n",
        "    model.train()\n",
        "    for batch in train_dl:\n",
        "        model.zero_grad()\n",
        "\n",
        "        inputs, targets = batch\n",
        "        inputs = inputs.long().to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        output = model(inputs)\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    all_predictions = torch.tensor([])\n",
        "    all_targets = torch.tensor([])\n",
        "    for batch in test_dl:\n",
        "        inputs, targets = batch\n",
        "        inputs = inputs.long().to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(inputs)\n",
        "\n",
        "        predictions = output.argmax(1)\n",
        "        all_targets = torch.cat([all_targets, targets.detach().cpu()])\n",
        "        all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n",
        "\n",
        "    val_acc = (all_predictions == all_targets).float().mean().numpy()\n",
        "    print(val_acc)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        torch.save(model.state_dict(), \"./model\")\n",
        "        torch.save(optimizer.state_dict(), \"./optimizer\")\n",
        "        best_val_acc = val_acc\n",
        "\n",
        "print(\"Best validation accuracy\", best_val_acc)"
      ],
      "metadata": {
        "id": "8jQzscISIdYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe23a1e-50b4-48a6-fcd1-6704791f73b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1\n",
            "0.5215\n",
            "Epoch #2\n",
            "0.7205\n",
            "Epoch #3\n",
            "0.517\n",
            "Epoch #4\n",
            "0.648\n",
            "Epoch #5\n",
            "0.5185\n",
            "Best validation accuracy 0.7205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector representation at word level\n"
      ],
      "metadata": {
        "id": "XdrDykrMHAwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
        "```\n",
        "\n",
        "In addition to the tokens present in our texts, we also add 2 special tokens: UNK (unknown word) and PAD.\n",
        "\n",
        "\n",
        "```\n",
        "Index assigned for every token: {'UNK': 0, 'PAD': 1, 'the': 2, 'mouse': 3, 'ran': 4, 'up': 5, 'clock': 6, 'down': 7}\n",
        "```\n",
        "\n",
        "The vector representation of the two texts using the corresponding index for each word:\n",
        "\n",
        "```\n",
        "'The mouse ran up the clock' = [2, 3, 4, 5, 2, 6]\n",
        "'The mouse ran down' = [2, 3, 4, 7]\n",
        "```\n",
        "\n",
        "We add padding values to the second vector to have a length equal to the first vector and we get:\n",
        "\n",
        "```\n",
        "[2, 3, 4, 7, 1]\n",
        "```\n",
        "One-hot representation of each text:\n",
        "\n",
        "```\n",
        "'The mouse ran up the clock' = [[0. 0. 1. 0. 0. 0. 0.]\n",
        "                                [0. 0. 0. 1. 0. 0. 0.]\n",
        "                                [0. 0. 0. 0. 1. 0. 0.]\n",
        "                                [0. 0. 0. 0. 0. 1. 0.]\n",
        "                                [0. 0. 1. 0. 0. 0. 0.]\n",
        "                                [0. 0. 0. 0. 0. 0. 1.]]\n",
        "\n",
        "'The mouse ran down' = [[0. 0. 1. 0. 0. 0. 0. 0.]\n",
        "                        [0. 0. 0. 1. 0. 0. 0. 0.]\n",
        "                        [0. 0. 0. 0. 1. 0. 0. 0.]\n",
        "                        [0. 0. 0. 0. 0. 0. 0. 1.]\n",
        "                        [0. 1. 0. 0. 0. 0. 0. 0.]]\n",
        "```"
      ],
      "metadata": {
        "id": "CXASSJD9E3wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercises\n",
        "\n",
        "1. Create the dictionary of unique words and their corresponding id for the train dataset. Same as before, we will use 0 for unknown, 1 for padding and the numbers from 2... for the other words.\n",
        "We also need to reduce the vocabulary. Some methods you can use:\n",
        "- Remove low frequency words\n",
        "- Remove very frequent words (stopwords)\n",
        "- Preprocess the text (stemming, lemmatization etc)\n",
        "2. Define a padding function. It should receive a text and a maximum length and\n",
        "  a. Truncate the text to the given length (if it's bigger)\n",
        "  b. Add the padding value to shorter texts, until they achieve the desired length\n",
        "3. Transform the train and test set into their vector representation -- replace each datapoint with a list of words, replace these words with their corresponding id, then pad each line to the desired length."
      ],
      "metadata": {
        "id": "N4_PllyUYQze"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMSq83igYQzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Exercises\n",
        "\n",
        "Compare the previous approaches on the given dataset. Try different paddings, architectures, preprocessing techniques etc. Display a table with your results"
      ],
      "metadata": {
        "id": "TLB4WZJBay3S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFes_sgGayO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}