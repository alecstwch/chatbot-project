# _page_2_Figure_8.md

**Figure 2: Distinguishing hyper plane to minimize the error**

**Image Analysis:** The image illustrates the concept of a Support Vector Machine (SVM) and how it finds the optimal hyperplane to separate data points of different classes. It consists of two sub-figures:

*   **Left:** Shows a scatter plot of two classes (red circles and green crosses) with multiple possible hyperplanes (dashed lines) separating them. The question "Which hyperplane?" is posed, indicating the problem of selecting the best separator.
*   **Right:** Demonstrates the SVM approach. It shows a "Decision boundary" (w<sup>T</sup>x = 0), which is the optimal hyperplane that maximizes the "Margin" between the two classes. Also shown are the "negative" hyperplane (w<sup>T</sup>x = -1) and the "positive" hyperplane (w<sup>T</sup>x = 1), which define the boundaries of the margin. The data points closest to these hyperplanes are marked as "Support vectors". The caption reads "SVM: Maximize the margin".

The image effectively visualizes the core principle of SVM: finding a hyperplane that not only separates the classes but also maximizes the distance to the nearest data points (support vectors) of each class.