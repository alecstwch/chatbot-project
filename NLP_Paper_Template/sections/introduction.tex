\section{Introduction}
\label{sec:introduction}

Conversational agents, commonly known as chatbots, have transformed human-computer interaction across numerous domains including healthcare, customer service, education, and entertainment. With the rise of transformer-based architectures and large language models, the landscape of chatbot development has evolved from simple rule-based systems to sophisticated neural approaches capable of generating contextually appropriate, human-like responses.

Despite these advances, fundamental questions remain: \textit{How do different architectural approaches compare in terms of performance, interpretability, and computational efficiency? Can hybrid systems combining symbolic and neural methods outperform pure neural approaches? What are the primary failure modes, and how can they be addressed?}

\subsection{Motivation}

This work is motivated by three key observations:

\begin{enumerate}
    \item \textbf{Healthcare accessibility}: Mental health support systems are critically needed but face significant resource constraints. Conversational agents can provide 24/7 support, though they must balance response quality with interpretability for clinical settings.
    
    \item \textbf{Architectural diversity}: The field lacks comprehensive comparisons between rule-based, neural, and hybrid approaches using modern NLP tools and evaluation frameworks.
    
    \item \textbf{Practical deployment}: Real-world chatbot applications require understanding trade-offs between model complexity, computational requirements, and performance metrics.
\end{enumerate}

\subsection{Research Questions}

This paper addresses the following research questions:

\begin{itemize}
    \item[\textbf{RQ1:}] How do rule-based (AIML), neural (DialoGPT), and hybrid (GPT-2 + Intent) approaches compare across multiple evaluation metrics?
    
    \item[\textbf{RQ2:}] What is the impact of explicit intent classification on response generation quality?
    
    \item[\textbf{RQ3:}] What are the primary error patterns, and how do they differ across architectural approaches?
    
    \item[\textbf{RQ4:}] What trade-offs exist between model interpretability, performance, and computational efficiency?
\end{itemize}

\subsection{Contributions}

Our main contributions are:

\begin{itemize}
    \item \textbf{Comprehensive evaluation framework}: We implement and evaluate three distinct chatbot architectures using modern NLP tools, providing detailed performance comparisons across classification (accuracy, precision, recall, F1) and generation (BLEU, ROUGE, METEOR) metrics.
    
    \item \textbf{Hybrid architecture}: We propose and validate a hybrid approach combining zero-shot intent classification (BART-large-MNLI) with GPT-2 generation, achieving superior performance (F1: 0.83, BLEU: 0.62) compared to pure rule-based and neural baselines.
    
    \item \textbf{Error analysis}: Through systematic error categorization and explainability studies (LIME), we identify that 60\% of errors stem from intent misclassification, providing actionable insights for future improvements.
    
    \item \textbf{Open-source implementation}: We release a complete, well-documented codebase following Domain-Driven Design principles, including preprocessing pipelines, model implementations, evaluation scripts, and MongoDB-based conversation storage.
\end{itemize}

\subsection{Paper Organization}

The remainder of this paper is organized as follows: Section~\ref{sec:related_work} reviews related work in conversational AI and dialogue systems. Section~\ref{sec:methodology} describes our datasets, preprocessing pipeline, and model architectures. Section~\ref{sec:results} presents comprehensive evaluation results. Section~\ref{sec:future_work} discusses future research directions. Section~\ref{sec:conclusion} concludes the paper. We also include sections on limitations (Section~\ref{sec:limitations}) and ethical considerations (Section~\ref{sec:ethical_statement}).
