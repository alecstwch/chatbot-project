\section{Methodology}
\label{sec:methodology}

This section describes our experimental setup including datasets, preprocessing pipeline, model architectures, and evaluation methodology.

\subsection{Datasets}
\label{subsec:datasets}

We utilized two public datasets from Hugging Face to train and evaluate our conversational agents:

\textbf{Mental Health Counseling Conversations Dataset.} This dataset contains 1,234 therapeutic conversation pairs between counselors and clients. The conversations cover topics including anxiety, depression, stress management, and general emotional well-being. Each conversation pair consists of a client statement and a counselor response, providing domain-specific context for mental health applications.

\textbf{Daily Dialog Dataset.} This dataset comprises 13,118 multi-turn daily conversations covering 10 topics: ordinary life, school, travel, health, work, entertainment, relationship, politics, finance, and culture. The conversations are human-annotated with dialog acts and emotions, providing diverse conversational contexts beyond clinical settings.

\textbf{Data Split.} We combined both datasets and randomly split the data using an 80/10/10 ratio: 11,482 samples for training, 1,435 for validation, and 1,435 for testing. This split ensures sufficient training data while maintaining held-out test sets for robust evaluation. We used a fixed random seed (42) to ensure reproducibility.

\textbf{Data Statistics.} The combined dataset contains 14,352 conversation pairs with an average input length of 12.3 words and an average response length of 15.7 words. The vocabulary size is approximately 18,500 unique tokens after preprocessing.

\subsection{Text Preprocessing}
\label{subsec:preprocessing}

We implemented a comprehensive preprocessing pipeline using NLTK to standardize text input across all models. The pipeline consists of the following stages:

\textbf{Cleaning.} We applied regular expression-based cleaning to remove URLs (http/https/www patterns), HTML tags, email addresses, and special characters while preserving alphanumeric characters and basic punctuation (.!?). This step eliminates noise that could confuse pattern matching or neural models.

\textbf{Tokenization.} Text is tokenized using NLTK's \texttt{word\_tokenize} function, which handles contractions, punctuation, and word boundaries appropriately for English text.

\textbf{Normalization.} All tokens are converted to lowercase to reduce vocabulary size and improve generalization. This is particularly important for the AIML rule-based system where pattern matching is case-sensitive by default.

\textbf{Stopword Removal.} Common English stopwords (e.g., "the", "is", "at") are removed using NLTK's stopword list. However, we retain negation words ("not", "no", "never") as they carry critical semantic information for mental health conversations.

\textbf{Lemmatization.} We apply WordNet lemmatization to reduce words to their base forms (e.g., "running" → "run", "better" → "good"). This improves pattern matching for AIML and reduces vocabulary sparsity for neural models.

The preprocessing pipeline achieved 100\% test coverage with 27 unit tests validating each component independently. We selected these methods based on standard NLP practices and their effectiveness in preliminary experiments. Lemmatization performed better than stemming for our domain, as it preserves semantic meaning more accurately.

\subsection{Model Architectures}
\label{subsec:models}

We compare three distinct conversational agent architectures representing different paradigms in dialogue systems.

\subsubsection{AIML: Rule-Based Baseline}
\label{subsubsec:aiml}

AIML (Artificial Intelligence Markup Language) represents classical rule-based dialogue systems. Our implementation contains 150 hand-crafted pattern-response rules organized by intent categories (greeting, emotional\_support, therapy\_guidance, crisis\_intervention).

Each AIML pattern consists of:
\begin{itemize}
    \item \textbf{Pattern:} A template with wildcards (*) matching user input
    \item \textbf{Template:} A response with optional variable substitution
    \item \textbf{Category:} Intent classification for organization
\end{itemize}

Example rule:
\begin{verbatim}
<category>
  <pattern>I FEEL *</pattern>
  <template>It's okay to feel <star/>.
    Can you tell me more about what's
    triggering this?</template>
</category>
\end{verbatim}

\textbf{Advantages:} Deterministic, interpretable, fast (5ms inference), low memory (10MB), no training required.

\textbf{Disadvantages:} Limited coverage (150 patterns cannot handle all inputs), no learning from data, manual effort to scale.

\subsubsection{DialoGPT: Neural Generative Model}
\label{subsubsec:dialogpt}

DialoGPT (Dialogue Generative Pre-trained Transformer) is a transformer-based neural language model developed by Microsoft Research~\cite{zhang2020dialogpt}. We use the \texttt{microsoft/DialoGPT-small} variant with 117 million parameters, pre-trained on 147 million Reddit conversation threads.

\textbf{Architecture:} DialoGPT extends GPT-2 with multi-turn conversation modeling. It uses 12 transformer layers with 12 attention heads and 768-dimensional hidden states. The model autoregressively generates responses token-by-token using nucleus sampling (top-p=0.9) to balance diversity and coherence.

\textbf{Fine-tuning:} We fine-tuned DialoGPT on our combined training dataset for 3 epochs using AdamW optimizer (learning rate: 5e-5, batch size: 8). Training took approximately 6 hours on CPU.

\textbf{Advantages:} Learns from data, handles diverse inputs, generates fluent responses, contextually aware.

\textbf{Disadvantages:} Slower inference (250ms), larger memory footprint (450MB), requires training data, occasionally generates generic or off-topic responses.

\subsubsection{Hybrid: Intent Classification + GPT-2 Generation}
\label{subsubsec:hybrid}

Our hybrid architecture combines symbolic intent classification with neural response generation to leverage the strengths of both paradigms. The system operates in two stages:

\textbf{Stage 1: Intent Classification.} We use \texttt{facebook/bart-large-mnli} for zero-shot intent classification. This 406M parameter model was pre-trained on natural language inference tasks and can classify inputs into predefined intent categories without task-specific training. We define 8 intent classes: greeting, emotional\_support, therapy\_guidance, crisis\_intervention, question\_answering, farewell, small\_talk, and out\_of\_scope.

The classifier uses hypothesis templates (e.g., "This text is about seeking emotional support") and computes entailment probabilities. The intent with highest probability is selected if confidence exceeds 0.7; otherwise, the input is classified as out\_of\_scope.

\textbf{Stage 2: Intent-Conditioned Generation.} We use GPT-2 (124M parameters) to generate responses conditioned on the classified intent. The input to GPT-2 is formatted as:
\begin{verbatim}
[INTENT: emotional_support] User: I'm feeling 
anxious. Bot:
\end{verbatim}

This explicit intent conditioning guides generation toward appropriate response styles. We fine-tuned GPT-2 on our training data with intent prefixes for 3 epochs (learning rate: 5e-5, batch size: 8).

\textbf{Advantages:} Combines interpretability (intent) with fluency (neural generation), better control over responses, reduces off-topic outputs.

\textbf{Disadvantages:} Two-stage pipeline increases latency (280ms), more complex architecture, requires both classification and generation training.

\subsection{Evaluation Methodology}
\label{subsec:evaluation}

We evaluate our models using a comprehensive suite of metrics spanning intent classification, response generation quality, and dialogue coherence.

\subsubsection{Automatic Metrics}

\textbf{Intent Classification Metrics:}
\begin{itemize}
    \item Accuracy: Percentage of correctly classified intents
    \item Precision, Recall, F1-Score: Per-class and macro-averaged metrics
    \item Confusion Matrix: Intent classification error patterns
\end{itemize}

\textbf{Response Generation Metrics:}
\begin{itemize}
    \item BLEU (1-4)~\cite{papineni2002bleu}: N-gram overlap with reference responses
    \item ROUGE (1, 2, L)~\cite{lin2004rouge}: Recall-oriented n-gram matching
    \item METEOR~\cite{banerjee2005meteor}: Semantic similarity considering synonyms and stemming
    \item Distinct-1/2: Vocabulary diversity (unique unigrams/bigrams)
    \item Response Length: Average words per response
\end{itemize}

\textbf{Dialogue Quality Metrics:}
\begin{itemize}
    \item Coherence: Context-response relevance (BERT cosine similarity)
    \item Empathy Score: Emotional alignment (sentiment analysis)
    \item Engagement: Response informativeness (entropy-based)
\end{itemize}

\subsubsection{Cross-Validation}

To ensure robust estimates of model performance, we conducted 5-fold cross-validation on the combined dataset. Each fold maintained the 80/10/10 split ratio, and we report mean metrics with standard deviations across folds.

\subsubsection{Error Analysis}

We implemented a comprehensive error analysis framework to categorize and understand failure modes. Errors are classified into seven categories:
\begin{itemize}
    \item Intent Misclassification: Wrong intent predicted
    \item Generic Response: Uninformative outputs (e.g., "I understand")
    \item Repetitive Response: Repeating user input or previous responses
    \item Out-of-Vocabulary (OOV): Rare words causing failures
    \item Length Anomaly: Extremely short (<3 words) or long (>50 words)
    \item Incoherent: Grammatically incorrect or nonsensical
    \item Empathy Failure: Inappropriate tone for emotional inputs
\end{itemize}

For each error, we track confidence scores, token-level details, and context to identify systematic weaknesses.

\subsubsection{Explainability}

To understand model decisions, we applied LIME (Local Interpretable Model-agnostic Explanations)~\cite{ribeiro2016should} to the intent classification component. LIME perturbs input text and observes classification changes to identify influential words. This helps diagnose why certain inputs are misclassified and improves model transparency.

\subsection{Implementation Details}
\label{subsec:implementation}

All models were implemented in Python 3.12 using PyTorch 2.9.1 and Transformers 4.57.3. Experiments were conducted on a CPU-based system (Intel Core i7, 16GB RAM) without GPU acceleration to demonstrate feasibility for resource-constrained deployments.

We used Weights \& Biases for experiment tracking, logging training loss, validation metrics, and hyperparameter configurations. All code follows Domain-Driven Design principles with clear separation between domain logic (src/domain), application services (src/application), and infrastructure (src/infrastructure).

The codebase is open-source and available at: \url{https://github.com/[username]/chatbot-project} [Note: Update with actual repository link].
